import numpy as np  # Importing NumPy for numerical operations
import tensorflow_datasets as tfds # Importing TensorFlow Datasets

import keras
import tensorflow as tf  # Importing TensorFlow for building the model
from sklearn.model_selection import train_test_split # Importing the train_test_split function
from sklearn.ensemble import RandomForestClassifier # Importing the Random Forest Classifier
from sklearn.metrics import accuracy_score # Importing the accuracy score function

# Load the Flower Photos dataset from Tensorflow Datasets
dataset, info = tfds.load('tf_flowers', with_info=True, as_supervised=True)

# Prepare the data for scikit-learn
def prepare_data(dataset):
  X = []
  y = []
  for image, label in dataset:
    X.append(tf.image.resize(image, (180, 180)).numpy().flatten())  # Resize and flatten images
    y.append(label.numpy())
  return np.array(X), np.array(y)

# Convert the datasets to NumPy arrays
X, y = prepare_data(dataset['train']) # Use the train split as there is no test split

# Get class names
class_names = info.features['label'].names

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)

# Create a RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # Fit the model on the training data  #

# Fit the model on the training data
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)  # Make predictions on the test set

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Random Forest Classifier on Flower Photos dataset: {accuracy:.2f}")

print(accuracy)


# Image preprocessing techniques for image classification:

# 1. Normalization:
#    Description: Scaling pixel values to a specific range, usually [0, 1] or [-1, 1].
#    How it works: Divides pixel values by the maximum possible value (255 for 8-bit images) or subtracts the mean and divides by the standard deviation.
#    Why it might improve performance: Helps gradient-based optimization algorithms converge faster and prevents dominance of features with larger values.
#    Applicability to tf_flowers: Useful as pixel values are currently in the range [0, 255].

# 2. Standardization:
#    Description: Scaling pixel values to have zero mean and unit variance.
#    How it works: Subtracts the mean of the pixel values from each pixel and divides by the standard deviation.
#    Why it might improve performance: Can be beneficial when the data has varying scales and distributions.
#    Applicability to tf_flowers: Could be explored, but normalization is generally more common for image data.

# 3. Data Augmentation:
#    Description: Creating new training examples by applying random transformations to the original images.
#    How it works: Techniques include rotation, zooming, shifting, flipping, and changing brightness or contrast.
#    Why it might improve performance: Increases the size and diversity of the training set, making the model more robust to variations in the input images and reducing overfitting.
#    Applicability to tf_flowers: Highly relevant as variations in pose, lighting, and scale are common in real-world images.

# 4. Resizing:
#    Description: Changing the dimensions of the images to a fixed size.
#    How it works: Interpolation techniques are used to add or remove pixels.
#    Why it might improve performance: Ensures consistent input size for the model and can reduce computational complexity.
#    Applicability to tf_flowers: Already applied in the current code (resizing to 180x180), but the size could be a hyperparameter to tune.

# 5. Grayscale Conversion:
#    Description: Converting color images to grayscale.
#    How it works: Typically by taking a weighted average of the R, G, and B channels.
#    Why it might improve performance: Can reduce the number of input features and potentially highlight structural information if color is not a crucial distinguishing factor.
#    Applicability to tf_flowers: May not be ideal as color is a significant feature for differentiating flower types.

# 6. Edge Detection:
#    Description: Identifying the boundaries of objects in the image.
#    How it works: Applying filters (e.g., Sobel, Canny) to highlight areas with sharp changes in pixel intensity.
#    Why it might improve performance: Can help the model focus on important structural features.
#    Applicability to tf_flowers: Could be an advanced technique to explore, potentially combined with other methods.

# Based on the task and the dataset, normalization and data augmentation are the most promising initial preprocessing techniques to investigate for improving the model's accuracy.

rom tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling

# Reshape the data for the CNN model
# CNNs expect input in the format (batch_size, height, width, channels)
# The images were originally resized to 180x180, and they are color images (3 channels)
X_train_cnn = X_train.reshape(-1, 180, 180, 3)
X_test_cnn = X_test.reshape(-1, 180, 180, 3)

# Build a simple CNN model
cnn_model = Sequential([
    Rescaling(1./255, input_shape=(180, 180, 3)), # Normalize pixel values to [0, 1]
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(class_names), activation='softmax') # Output layer with number of classes
])

# Compile the model
cnn_model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])

# Train the CNN model
# Using a small number of epochs for demonstration; more epochs may be needed for better accuracy
history = cnn_model.fit(X_train_cnn, y_train, epochs=5, validation_data=(X_test_cnn, y_test))

# Evaluate the CNN model
loss, accuracy_cnn = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)

print(f"Accuracy of CNN on Flower Photos dataset: {accuracy_cnn:.2f}")


