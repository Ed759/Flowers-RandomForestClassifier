import numpy as np  # Importing NumPy for numerical operations
import tensorflow_datasets as tfds # Importing TensorFlow Datasets

import keras
import tensorflow as tf  # Importing TensorFlow for building the model
from sklearn.model_selection import train_test_split # Importing the train_test_split function
from sklearn.ensemble import RandomForestClassifier # Importing the Random Forest Classifier
from sklearn.metrics import accuracy_score # Importing the accuracy score function

# Load the Flower Photos dataset from Tensorflow Datasets
dataset, info = tfds.load('tf_flowers', with_info=True, as_supervised=True)

# Prepare the data for scikit-learn
def prepare_data(dataset):
  X = []
  y = []
  for image, label in dataset:
    X.append(tf.image.resize(image, (180, 180)).numpy().flatten())  # Resize and flatten images
    y.append(label.numpy())
  return np.array(X), np.array(y)

# Convert the datasets to NumPy arrays
X, y = prepare_data(dataset['train']) # Use the train split as there is no test split

# Get class names
class_names = info.features['label'].names

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)

# Create a RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # Fit the model on the training data  #

# Fit the model on the training data
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)  # Make predictions on the test set

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Random Forest Classifier on Flower Photos dataset: {accuracy:.2f}")

print(accuracy)


# Image preprocessing techniques for image classification:

# 1. Normalization:
#    Description: Scaling pixel values to a specific range, usually [0, 1] or [-1, 1].
#    How it works: Divides pixel values by the maximum possible value (255 for 8-bit images) or subtracts the mean and divides by the standard deviation.
#    Why it might improve performance: Helps gradient-based optimization algorithms converge faster and prevents dominance of features with larger values.
#    Applicability to tf_flowers: Useful as pixel values are currently in the range [0, 255].

# 2. Standardization:
#    Description: Scaling pixel values to have zero mean and unit variance.
#    How it works: Subtracts the mean of the pixel values from each pixel and divides by the standard deviation.
#    Why it might improve performance: Can be beneficial when the data has varying scales and distributions.
#    Applicability to tf_flowers: Could be explored, but normalization is generally more common for image data.

# 3. Data Augmentation:
#    Description: Creating new training examples by applying random transformations to the original images.
#    How it works: Techniques include rotation, zooming, shifting, flipping, and changing brightness or contrast.
#    Why it might improve performance: Increases the size and diversity of the training set, making the model more robust to variations in the input images and reducing overfitting.
#    Applicability to tf_flowers: Highly relevant as variations in pose, lighting, and scale are common in real-world images.

# 4. Resizing:
#    Description: Changing the dimensions of the images to a fixed size.
#    How it works: Interpolation techniques are used to add or remove pixels.
#    Why it might improve performance: Ensures consistent input size for the model and can reduce computational complexity.
#    Applicability to tf_flowers: Already applied in the current code (resizing to 180x180), but the size could be a hyperparameter to tune.

# 5. Grayscale Conversion:
#    Description: Converting color images to grayscale.
#    How it works: Typically by taking a weighted average of the R, G, and B channels.
#    Why it might improve performance: Can reduce the number of input features and potentially highlight structural information if color is not a crucial distinguishing factor.
#    Applicability to tf_flowers: May not be ideal as color is a significant feature for differentiating flower types.

# 6. Edge Detection:
#    Description: Identifying the boundaries of objects in the image.
#    How it works: Applying filters (e.g., Sobel, Canny) to highlight areas with sharp changes in pixel intensity.
#    Why it might improve performance: Can help the model focus on important structural features.
#    Applicability to tf_flowers: Could be an advanced technique to explore, potentially combined with other methods.

# Based on the task and the dataset, normalization and data augmentation are the most promising initial preprocessing techniques to investigate for improving the model's accuracy.

rom tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling

# Reshape the data for the CNN model
# CNNs expect input in the format (batch_size, height, width, channels)
# The images were originally resized to 180x180, and they are color images (3 channels)
X_train_cnn = X_train.reshape(-1, 180, 180, 3)
X_test_cnn = X_test.reshape(-1, 180, 180, 3)

# Build a simple CNN model
cnn_model = Sequential([
    Rescaling(1./255, input_shape=(180, 180, 3)), # Normalize pixel values to [0, 1]
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(class_names), activation='softmax') # Output layer with number of classes
])

# Compile the model
cnn_model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])

# Train the CNN model
# Using a small number of epochs for demonstration; more epochs may be needed for better accuracy
history = cnn_model.fit(X_train_cnn, y_train, epochs=5, validation_data=(X_test_cnn, y_test))

# Evaluate the CNN model
loss, accuracy_cnn = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)

print(f"Accuracy of CNN on Flower Photos dataset: {accuracy_cnn:.2f}")

# Tune Hyperparameters

from sklearn.model_selection import ParameterSampler, train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling
from tensorflow.keras.optimizers import Adam
import gc # Import garbage collector
import keras # Import keras
import tensorflow as tf # Import tensorflow for losses
import numpy as np # Import numpy
import tensorflow_datasets as tfds # Import tensorflow_datasets

# Load the Flower Photos dataset from Tensorflow Datasets
dataset, info = tfds.load('tf_flowers', with_info=True, as_supervised=True)

# Prepare the data for scikit-learn
def prepare_data(dataset):
  X = []
  y = []
  for image, label in dataset:
    X.append(tf.image.resize(image, (180, 180)).numpy().flatten())  # Resize and flatten images
    y.append(label.numpy())
  return np.array(X), np.array(y)

# Convert the datasets to NumPy arrays
X, y = prepare_data(dataset['train']) # Use the train split as there is no test split

# Get class names
class_names = info.features['label'].names

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)

# Reshape the data for the CNN model
# CNNs expect input in the format (batch_size, height, width, channels)
# The images were originally resized to 180x180, and they are color images (3 channels)
X_train_cnn = X_train.reshape(-1, 180, 180, 3)
X_test_cnn = X_test.reshape(-1, 180, 180, 3)


# Define the hyperparameter search space for random search
param_dist = {
    'conv_filters': [(32, 64, 128), (64, 128, 256)],
    'dense_neurons': [64, 128, 256],
    'learning_rate': [0.001, 0.0001]
}

# Number of random combinations to try
n_iter_search = 5

# Create a parameter sampler
random_search = ParameterSampler(param_dist, n_iter=n_iter_search, random_state=42)

best_accuracy = 0
best_params = {}

# Iterate over the random parameter combinations
for i, params in enumerate(random_search):
    print(f"Training model with parameters: {params}")

    # Clear previous model and free up memory
    keras.backend.clear_session()
    gc.collect()

    # Build the CNN model with current hyperparameters
    model = Sequential([
        Rescaling(1./255, input_shape=(180, 180, 3)),
        Conv2D(params['conv_filters'][0], (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Conv2D(params['conv_filters'][1], (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Conv2D(params['conv_filters'][2], (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(params['dense_neurons'], activation='relu'),
        Dense(len(class_names), activation='softmax')
    ])

    # Compile the model with the specified learning rate
    optimizer = Adam(learning_rate=params['learning_rate'])
    model.compile(optimizer=optimizer,
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])

    # Train the model (using a small number of epochs for hyperparameter tuning)
    history = model.fit(X_train_cnn, y_train, epochs=3, validation_data=(X_test_cnn, y_test), verbose=0)

    # Evaluate the model
    loss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)
    print(f"Accuracy for these parameters: {accuracy:.4f}")

    # Check if this is the best model so far
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_params = params

print("\nBest parameters found:")
print(best_params)
print(f"Best accuracy: {best_accuracy:.4f}")

# Consider using Pre-Trained Models

# Step 1 & 2: Choose and load a pre-trained model (MobileNetV2)
# MobileNetV2 is a good choice for mobile and embedded vision applications due to its efficiency.
# We exclude the top classification layer and load weights from ImageNet.
base_model = tf.keras.applications.MobileNetV2(input_shape=(180, 180, 3),
                                               include_top=False,
                                               weights='imagenet')

# Step 4: Freeze the base model's layers
# This prevents the weights of the pre-trained layers from being updated during training.
base_model.trainable = False

# Step 3: Add new layers on top of the base
# Use Functional API to build the new model
inputs = tf.keras.Input(shape=(180, 180, 3))
x = base_model(inputs, training=False) # Set training to False when using the base model with BatchNormalization
x = tf.keras.layers.GlobalAveragePooling2D()(x) # Use GlobalAveragePooling to reduce dimensions
x = tf.keras.layers.Dropout(0.2)(x) # Add dropout for regularization
outputs = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

# Step 5: Compile the new model
# Use Adam optimizer with a small learning rate, suitable for fine-tuning
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

# Prepare data for the new model (ensure it's in the correct shape and scaled)
# The base model expects input in the range [0, 255] or scaled, depending on the model.
# MobileNetV2 expects input scaled to [-1, 1] or [0, 1]. Rescaling to [0, 1] is simpler here.
X_train_cnn_scaled = X_train_cnn / 255.0
X_test_cnn_scaled = X_test_cnn / 255.0


# Step 6: Train the new model
# Train on the prepared data with validation
epochs = 10 # Start with a small number of epochs
history_fine_tune = model.fit(X_train_cnn_scaled, y_train,
                              epochs=epochs,
                              validation_data=(X_test_cnn_scaled, y_test))

# Step 7: Evaluate the fine-tuned model
loss_fine_tune, accuracy_fine_tune = model.evaluate(X_test_cnn_scaled, y_test, verbose=0)

print(f"Accuracy of fine-tuned MobileNetV2 on Flower Photos dataset: {accuracy_fine_tune:.2f}")

# Evaluate and Iterate

import tensorflow as tf
import numpy as np  # Importing NumPy for numerical operations
import tensorflow_datasets as tfds # Importing TensorFlow Datasets
from sklearn.model_selection import train_test_split # Importing the train_test_split function


# Load the Flower Photos dataset from Tensorflow Datasets
dataset, info = tfds.load('tf_flowers', with_info=True, as_supervised=True)

# Prepare the data for scikit-learn
def prepare_data(dataset):
  X = []
  y = []
  for image, label in dataset:
    X.append(tf.image.resize(image, (180, 180)).numpy().flatten())  # Resize and flatten images
    y.append(label.numpy())
  return np.array(X), np.array(y)

# Convert the datasets to NumPy arrays
X, y = prepare_data(dataset['train']) # Use the train split as there is no test split

# Get class names
class_names = info.features['label'].names

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)

# Reshape the data for the CNN model
# CNNs expect input in the format (batch_size, height, width, channels)
# The images were originally resized to 180x180, and they are color images (3 channels)
X_train_cnn = X_train.reshape(-1, 180, 180, 3)
X_test_cnn = X_test.reshape(-1, 180, 180, 3)

X_train_cnn_scaled = X_train_cnn / 255.0
X_test_cnn_scaled = X_test_cnn / 255.0

# Step 1 & 2: Choose and load a pre-trained model (MobileNetV2)
# MobileNetV2 is a good choice for mobile and embedded vision applications due to its efficiency.
# We exclude the top classification layer and load weights from ImageNet.
base_model = tf.keras.applications.MobileNetV2(input_shape=(180, 180, 3),
                                               include_top=False,
                                               weights='imagenet')

# Step 4: Freeze the base model's layers
# This prevents the weights of the pre-trained layers from being updated during training.
base_model.trainable = False

# Step 3: Add new layers on top of the base
# Use Functional API to build the new model
inputs = tf.keras.Input(shape=(180, 180, 3))
x = base_model(inputs, training=False) # Set training to False when using the base model with BatchNormalization
x = tf.keras.layers.GlobalAveragePooling2D()(x) # Use GlobalAveragePooling to reduce dimensions
x = tf.keras.layers.Dropout(0.2)(x) # Add dropout for regularization
outputs = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

# Step 5: Compile the new model
# Use Adam optimizer with a small learning rate, suitable for fine-tuning
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

# Step 6: Train the new model
# Train on the prepared data with validation
epochs = 10 # Start with a small number of epochs
history_fine_tune = model.fit(X_train_cnn_scaled, y_train,
                              epochs=epochs,
                              validation_data=(X_test_cnn_scaled, y_test))

# Step 7: Evaluate the fine-tuned model
loss_fine_tune, accuracy_fine_tune = model.evaluate(X_test_cnn_scaled, y_test, verbose=0)

print(f"Accuracy of fine-tuned MobileNetV2 on Flower Photos dataset: {accuracy_fine_tune:.2f}")


# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Random Forest Classifier on Flower Photos dataset: {accuracy:.2f}")

print(accuracy)

# Save the best performing model
model.save('flower_classification_model.h5')
